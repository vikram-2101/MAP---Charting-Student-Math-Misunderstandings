# --- Setup and Imports ---
!pip install pandas scikit-learn transformers sentence-transformers faiss-cpu rank-bm25 datasets torch matplotlib seaborn --quiet
!pip install accelerate -U --quiet # Update accelerate for Hugging Face Trainer

import pandas as pd
import numpy as np
import os
import re
import random
import torch
from torch.utils.data import Dataset, DataLoader
from torch.nn import BCEWithLogitsLoss

# Hugging Face and PyTorch
from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, DataCollatorForLanguageModeling
from sentence_transformers import SentenceTransformer
# FAISS and BM25
import faiss
from rank_bm25 import BM25Okapi
# Metrics
from sklearn.metrics import precision_recall_curve, auc, average_precision_score, f1_score
# Visualization (for evaluation)
import matplotlib.pyplot as plt

# Set up device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Set the path to your data files in Google Drive
data_path = "/content/" # CHANGE THIS TO YOUR FOLDER PATH

# --- 1. Data Pipeline: Ingestion, Normalization, and Math-Aware Cleaning ---

# Load the datasets
try:
    train_df = pd.read_csv(os.path.join(data_path, 'train.csv'))
    val_df = pd.read_csv(os.path.join(data_path, 'test.csv'))
except FileNotFoundError:
    print("WARNING: Data files not found. Using dummy data for structural testing.")
    # Create dummy data if files are missing for structure testing
    train_data = {
        'QuestionText': ["What is 1/2 + 1/3?", "Compare 0.5 and 0.25."],
        'StudentExplanation': ["I added 1+1 and 2+3 to get 2/5.", "I think 0.5 is bigger than 0.25 because 5 is bigger than 25."],
        'Category': ['Additive', 'Decimal_Comparison_Whole_Numbers'],
        'Misconception': [np.nan, 'Procedural']
    }
    val_data = {
        'QuestionText': ["What is 3/4 - 1/4?", "Compare 0.1 and 0.05."],
        'StudentExplanation': ["The answer is 2/4 which is 1/2.", "The longer number is bigger, so 0.05 is bigger than 0.1."],
        'Category': ['True_Correct', 'Decimal_Comparison_Whole_Numbers'],
        'Misconception': [np.nan, np.nan]
    }
    train_df = pd.DataFrame(train_data)
    val_df = pd.DataFrame(val_data)


# Combine text fields
train_df['text'] = train_df['QuestionText'] + " " + train_df['StudentExplanation'].fillna('')
val_df['text'] = val_df['QuestionText'] + " " + val_df['StudentExplanation'].fillna('')

# Math-aware text cleaning (more robust than original)
def clean_text(text):
    text = text.lower()
    # Math-aware cleanup: Preserve symbols and numbers
    # Normalize fractions (e.g., 1 / 2 -> 1/2) and decimals (e.g., 0.80 -> 0.8)
    text = re.sub(r'\s*/\s*', '/', text) # Unify fraction spacing
    text = re.sub(r'(\d+\.\d+?)\d+$', r'\1', text) # Simple attempt to unify decimals (e.g., 0.80 -> 0.8)
    # General normalization
    text = re.sub(r'\s+', ' ', text).strip()
    return text

train_df['cleaned_text'] = train_df['text'].apply(clean_text)
val_df['cleaned_text'] = val_df['text'].apply(clean_text)

print(f"Total training samples: {len(train_df)}")

# --- 2. Misconception Knowledge Store & Label Management ---

# Create the canonical list of misconception labels
all_unique_labels = sorted(list(set(train_df['Category'].dropna()).union(set(train_df['Misconception'].dropna()))))

# Rich descriptions (manually curated - as per methodology)
rich_misconception_descriptions = {
    "Decimal_Comparison_Whole_Numbers": "The student is incorrectly comparing decimals by ignoring the decimal point and treating the numbers as if they were whole numbers. They may also think that a longer decimal is always a larger number.",
    "Fraction_Visual_Incorrect": "The student has a misconception based on a visual representation of a fraction, perhaps by miscounting the total number of parts or shaded parts from an image.",
    "True_Correct": "The student's response is correct and does not contain a misconception."
}

# Ensure all labels have a unique ID and description
misconception_knowledge_store = {}
for i, label in enumerate(all_unique_labels):
    description = rich_misconception_descriptions.get(
        label,
        f"This response demonstrates a mathematical issue related to the concept: {label}." # Fallback description
    )
    misconception_knowledge_store[label] = {
        "id": i,
        "label": label,
        "description": description,
        "embedding": None
    }
all_unique_labels = sorted(list(misconception_knowledge_store.keys())) # Re-sort to ensure index consistency

# Map IDs to labels for FAISS retrieval
id_to_label = {data['id']: label for label, data in misconception_knowledge_store.items()}


# --- Mock Domain Adaptation Pre-training (DAPT) Step ---
# NOTE: Full DAPT requires a large external corpus (math Q&A) and significant training.
# This section only demonstrates the *required structure* before fine-tuning.

print("\n--- Mock Domain Adaptation Pre-training Step ---")
# 1. Prepare an auxiliary dataset (e.g., all student texts) for Masked Language Modeling (MLM)
dapt_texts = train_df['cleaned_text'].tolist() + val_df['cleaned_text'].tolist()
# 2. Mock: Save these texts to a file as if for a standard Hugging Face DAPT run
with open("dapt_corpus.txt", "w") as f:
    for text in dapt_texts:
        f.write(text + "\n")
print(f"Mock DAPT corpus file created with {len(dapt_texts)} lines.")

# In a real environment, you would run a script like:
# !python run_mlm.py --model_name_or_path bert-base-uncased --train_file dapt_corpus.txt --output_dir ./dapt_model

# For simplicity, we will continue with the base model, but we use the output from this step:
# bi_encoder_name = "./dapt_model" # If DAPT was run successfully
bi_encoder_name = 'all-MiniLM-L6-v2' # Fallback to base
cross_encoder_name = "cross-encoder/ms-marco-TinyBERT-L-2-v2"

# --- 3. Stage A: Candidate Retrieval Setup (Bi-encoder + FAISS + BM25) ---

print("\nLoading bi-encoder model and generating embeddings...")
bi_encoder = SentenceTransformer(bi_encoder_name).to(device)

# Generate embeddings
misconception_texts = [entry['description'] for entry in misconception_knowledge_store.values()]
misconception_embeddings = bi_encoder.encode(misconception_texts, convert_to_tensor=True, show_progress_bar=False)

# Store the embeddings and build FAISS index
for label, embedding in zip(misconception_knowledge_store.keys(), misconception_embeddings):
    misconception_knowledge_store[label]['embedding'] = embedding.cpu().numpy()

dimension = misconception_embeddings.shape[1]
index = faiss.IndexFlatL2(dimension)
index.add(misconception_embeddings.cpu().numpy())

print(f"FAISS index created with {index.ntotal} embeddings.")

# BM25 setup
corpus = [data['description'] for data in misconception_knowledge_store.values()]
tokenized_corpus = [doc.split(" ") for doc in corpus]
bm25 = BM25Okapi(tokenized_corpus)

def hybrid_search(query_text, k=10, alpha=0.7):
    # Dense (FAISS)
    query_embedding = bi_encoder.encode(query_text, convert_to_tensor=True).reshape(1, -1)
    faiss_distances, faiss_indices = index.search(query_embedding.cpu().numpy(), k)

    # Normalize distances to scores (0 to 1, where 1 is best)
    max_dist = np.max(faiss_distances)
    min_dist = np.min(faiss_distances)
    # Handle case where all distances are identical
    if max_dist == min_dist:
        dense_scores = np.ones_like(faiss_distances[0])
    else:
        dense_scores = 1 - (faiss_distances[0] - min_dist) / (max_dist - min_dist + 1e-6)
    dense_scores_dict = {faiss_indices[0][i]: dense_scores[i] for i in range(k)}

    # BM25 (lexical)
    tokenized_query = query_text.split(" ")
    bm25_scores = bm25.get_scores(tokenized_query)
    top_bm25_indices = np.argsort(bm25_scores)[::-1] # Check all indices for combination

    max_bm25_score = np.max(bm25_scores)
    if max_bm25_score <= 0:
        bm25_scores_dict = {idx: 0 for idx in range(len(bm25_scores))}
    else:
        bm25_scores_dict = {idx: bm25_scores[idx] / max_bm25_score for idx in range(len(bm25_scores))}

    # Combine
    final_scores = {}
    # Only consider indices that were in the top K of either search to limit overhead
    all_indices = set(list(dense_scores_dict.keys()) + list(top_bm25_indices[:k]))

    for doc_id in all_indices:
        dense_score = dense_scores_dict.get(doc_id, 0)
        bm25_score = bm25_scores_dict.get(doc_id, 0)
        final_scores[doc_id] = alpha * dense_score + (1 - alpha) * bm25_score

    sorted_scores = sorted(final_scores.items(), key=lambda x: x[1], reverse=True)

    retrieved_results = []
    for doc_id, score in sorted_scores[:k]:
        label = id_to_label[doc_id]
        description = misconception_knowledge_store[label]['description']
        retrieved_results.append({'label': label, 'id': doc_id, 'score': score, 'description': description})

    return retrieved_results

# --- 4. Stage B: Cross-Encoder Data Preparation and Hard-Negative Mining ---

cross_encoder_tokenizer = AutoTokenizer.from_pretrained(cross_encoder_name)
cross_encoder_model = AutoModelForSequenceClassification.from_pretrained(cross_encoder_name, num_labels=1).to(device)

train_examples = []
val_examples = []
num_hard_negatives = 3
label_to_desc = {label: data['description'] for label, data in misconception_knowledge_store.items()}

def create_examples(df, examples_list, is_training):
    for _, row in df.iterrows():
        student_response = row['cleaned_text']

        # Get the true positive label(s)
        positive_labels = []
        if pd.notna(row['Category']):
            positive_labels.append(row['Category'])
        if pd.notna(row['Misconception']):
            positive_labels.append(row['Misconception'])
        # Remove true/neutral labels from positive set if we focus on misconceptions
        positive_labels = [l for l in positive_labels if not l.startswith('True_')]

        # Positive examples (label=1.0)
        for pos_label in positive_labels:
            if pos_label in label_to_desc:
                pos_desc = label_to_desc[pos_label]
                examples_list.append({'text_a': student_response, 'text_b': pos_desc, 'label': 1.0})
                
                # Simple Label-Preserving Augmentation (for training only)
                if is_training:
                    augmented_texts = augment_response(student_response, pos_label)
                    for aug_text in augmented_texts:
                        examples_list.append({'text_a': aug_text, 'text_b': pos_desc, 'label': 1.0})


        # Hard Negative Mining (label=0.0)
        if positive_labels: # Only mine negatives if there's a positive label to contrast against
            retrieved_candidates = hybrid_search(student_response, k=10, alpha=0.7)
            
            negative_labels = [
                candidate['label'] for candidate in retrieved_candidates
                if candidate['label'] not in positive_labels and not candidate['label'].startswith('True_')
            ]

            # Sample hard negatives
            hard_negatives = random.sample(negative_labels, min(len(negative_labels), num_hard_negatives))

            for neg_label in hard_negatives:
                neg_desc = label_to_desc[neg_label]
                examples_list.append({'text_a': student_response, 'text_b': neg_desc, 'label': 0.0})

# Augmentation Function (as per methodology)
def augment_response(text, label):
    augmented_examples = []
    # Augmentation for Decimal_Comparison_Whole_Numbers
    if label == "Decimal_Comparison_Whole_Numbers":
        original_numbers = re.findall(r'(\d+\.\d+|\d+)', text)
        if len(original_numbers) >= 2 and '.' in original_numbers[0] and '.' in original_numbers[1]:
             # Create new version with different numbers but the same conceptual error
            new_num1 = round(random.uniform(0.1, 0.4), 2)
            new_num2 = round(random.uniform(0.5, 0.9), 2)
            # Ensure the WHOLE NUMBER comparison is wrong (e.g., 0.1 < 0.9 but 10 < 90)
            if int(new_num1 * 100) < int(new_num2 * 100):
                 new_text = f"I think {new_num2} is bigger than {new_num1} because {int(new_num2*100)} is bigger than {int(new_num1*100)}."
                 augmented_examples.append(clean_text(new_text))
    return augmented_examples

# Create the training and validation data for the cross-encoder
create_examples(train_df, train_examples, is_training=True)
create_examples(val_df, val_examples, is_training=False) # Use validation data as it should be

print(f"Total training examples for cross-encoder: {len(train_examples)}")
print(f"Total validation examples for cross-encoder: {len(val_examples)}")


# Custom Dataset Class
class MisconceptionDataset(Dataset):
    def __init__(self, examples, tokenizer):
        self.examples = examples
        self.tokenizer = tokenizer

    def __len__(self):
        return len(self.examples)

    def __getitem__(self, idx):
        example = self.examples[idx]
        tokenized_output = self.tokenizer(example['text_a'], example['text_b'], 
                                          padding='max_length', truncation=True, 
                                          max_length=128, return_tensors="pt")

        return {
            'input_ids': tokenized_output['input_ids'].squeeze(),
            'attention_mask': tokenized_output['attention_mask'].squeeze(),
            'labels': torch.tensor(example['label'], dtype=torch.float)
        }

train_dataset = MisconceptionDataset(train_examples, cross_encoder_tokenizer)
val_dataset = MisconceptionDataset(val_examples, cross_encoder_tokenizer)

# --- 5. Stage B: Cross-Encoder Fine-Tuning with Weighted Loss ---

# Calculate class weights for imbalance
positive_count = sum(1 for ex in train_examples if ex['label'] == 1.0)
negative_count = sum(1 for ex in train_examples if ex['label'] == 0.0)
pos_weight_value = negative_count / positive_count if positive_count > 0 else 1.0

# Custom Trainer for Weighted BCE Loss
class WeightedTrainer(Trainer):
    def compute_loss(self, model, inputs, return_outputs=False):
        labels = inputs.pop("labels")
        outputs = model(**inputs)
        logits = outputs.get("logits")
        labels = labels.squeeze()

        # Use the calculated weight
        loss_fct = BCEWithLogitsLoss(pos_weight=torch.tensor(pos_weight_value, device=model.device))
        loss = loss_fct(logits.squeeze(), labels)

        return (loss, outputs) if return_outputs else loss

training_args = TrainingArguments(
    output_dir="./cross_encoder_results",
    num_train_epochs=3,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    learning_rate=2e-5,
    eval_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
    report_to="none", # simplified logging
    fp16=torch.cuda.is_available() # Enable FP16 if CUDA is available
)

trainer = WeightedTrainer(
    model=cross_encoder_model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
)

print("\nStarting cross-encoder fine-tuning with weighted loss...")
trainer.train()
cross_encoder_model.save_pretrained("./final_cross_encoder")
cross_encoder_tokenizer.save_pretrained("./final_cross_encoder")

# --- 6. Evaluation and Threshold Calibration (Crucial Missing Step) ---

# --- A. Collect all predictions and true labels on validation set ---
def evaluate_and_calibrate(eval_df):
    
    # 1. Prepare data for evaluation: retrieve candidates for ALL validation samples
    all_scores = []
    all_true_labels = []

    # Map validation labels to a multi-hot vector (for evaluation)
    all_labels_set = set(all_unique_labels)
    label_to_idx = {label: i for i, label in enumerate(all_unique_labels)}

    for _, row in eval_df.iterrows():
        student_response = row['cleaned_text']
        true_labels = []
        if pd.notna(row['Category']):
            true_labels.append(row['Category'])
        if pd.notna(row['Misconception']):
            true_labels.append(row['Misconception'])

        # Multi-hot vector for ground truth
        true_vector = np.zeros(len(all_unique_labels))
        for label in true_labels:
            if label in label_to_idx:
                true_vector[label_to_idx[label]] = 1.0
        
        # 2. Stage A: Hybrid Retrieval
        retrieved_candidates = hybrid_search(student_response, k=20, alpha=0.7) # K=20 for robust evaluation
        
        retrieved_labels = [candidate['label'] for candidate in retrieved_candidates]
        reranking_pairs = [[student_response, misconception_knowledge_store[label]['description']] for label in retrieved_labels]

        # 3. Stage B: Cross-Encoder Scoring
        with torch.no_grad():
            tokenized_output = cross_encoder_tokenizer(reranking_pairs, padding=True, truncation=True, return_tensors="pt").to(device)
            reranking_scores = cross_encoder_model(**tokenized_output).logits.squeeze().cpu().numpy()
            probabilities = torch.sigmoid(torch.tensor(reranking_scores)).numpy()
        
        # 4. Map scores back to the full label space
        prediction_vector = np.zeros(len(all_unique_labels))
        for i, label in enumerate(retrieved_labels):
            if label in label_to_idx:
                prediction_vector[label_to_idx[label]] = probabilities[i] # Store the probability

        all_scores.append(prediction_vector)
        all_true_labels.append(true_vector)
    
    all_scores = np.array(all_scores)
    all_true_labels = np.array(all_true_labels)
    
    return all_scores, all_true_labels

val_scores, val_true_labels = evaluate_and_calibrate(val_df)

# --- B. Per-Label Threshold Calibration (PR-Optimal F1) ---

def find_best_threshold(y_true, y_scores):
    # Calculate precision and recall for all thresholds
    precision, recall, thresholds = precision_recall_curve(y_true, y_scores)
    
    # Calculate F1-score for all thresholds
    fscore = (2 * precision * recall) / (precision + recall + 1e-6)
    
    # Find the threshold that maximizes F1-score (PR-Optimal)
    ix = np.argmax(fscore)
    return thresholds[ix]

per_label_thresholds = {}
macro_f1_scores = []
avg_precision_scores = []
map_k_scores = [] # MAP@K calculation is more complex, focusing on F1/AUC-PR here

print("\n--- Evaluation and Threshold Calibration ---")
for i, label in enumerate(all_unique_labels):
    y_true = val_true_labels[:, i]
    y_scores = val_scores[:, i]
    
    # Skip labels with no positive examples in validation
    if np.sum(y_true) == 0:
        per_label_thresholds[label] = 0.5 # Default fallback
        continue
    
    # Find PR-Optimal Threshold
    best_threshold = find_best_threshold(y_true, y_scores)
    per_label_thresholds[label] = best_threshold
    
    # Calculate Macro F1 per label (using the best threshold for binary prediction)
    y_pred_binary = (y_scores >= best_threshold).astype(int)
    f1 = f1_score(y_true, y_pred_binary, zero_division=0)
    
    # Calculate AUC-PR (Average Precision)
    ap = average_precision_score(y_true, y_scores)

    macro_f1_scores.append(f1)
    avg_precision_scores.append(ap)
    
    print(f"Label: {label:<40} | Best Thresh: {best_threshold:.4f} | F1: {f1:.4f} | AUC-PR: {ap:.4f}")

# Overall Macro Metrics
overall_macro_f1 = np.mean(macro_f1_scores)
overall_auc_pr = np.mean(avg_precision_scores)
print(f"\nOverall Macro F1 (Calibrated Thresh): {overall_macro_f1:.4f}")
print(f"Overall Macro AUC-PR: {overall_auc_pr:.4f}")

# --- 7. Final Inference Pipeline with Calibrated Thresholds ---

def predict_misconceptions_final(student_response, k=10, alpha=0.7, thresholds=per_label_thresholds):
    """
    End-to-end pipeline using calibrated thresholds.
    """
    cleaned_response = clean_text(student_response)

    # 1. Stage A: Hybrid Candidate Retrieval
    retrieved_candidates = hybrid_search(cleaned_response, k=k, alpha=alpha)

    # 2. Stage B: Cross-Encoder Re-ranking
    reranking_pairs = []
    retrieved_labels = [candidate['label'] for candidate in retrieved_candidates]
    for label in retrieved_labels:
        description = misconception_knowledge_store[label]['description']
        reranking_pairs.append([cleaned_response, description])

    with torch.no_grad():
        tokenized_output = cross_encoder_tokenizer(reranking_pairs, padding=True, truncation=True, return_tensors="pt").to(device)
        reranking_scores = cross_encoder_model(**tokenized_output).logits.squeeze().cpu().numpy()

    scored_candidates = {
        retrieved_labels[i]: reranking_scores[i] for i in range(len(retrieved_labels))
    }

    # 3. Final Thresholding using Per-Label Calibrated Thresholds
    final_predictions = []
    probabilities = torch.sigmoid(torch.tensor(list(scored_candidates.values())))

    for i, label in enumerate(scored_candidates.keys()):
        score = probabilities[i].item()
        
        # Use the calibrated threshold for this specific label
        threshold = thresholds.get(label, 0.5) 
        
        if score > threshold:
            final_predictions.append({
                'label': label,
                'score': score,
                'threshold': threshold,
                'description': misconception_knowledge_store[label]['description']
            })

    final_predictions = sorted(final_predictions, key=lambda x: x['score'], reverse=True)

    return final_predictions

# --- Test the final complete pipeline ---
test_response_1 = "I think 0.5 is bigger than 0.75 because 5 is bigger than 75."
test_response_2 = "The answer is 5/8 because I added 2 and 3, and 4 and 4."

print("\n\n--- Testing Final Pipeline ---")
print(f"Student Response 1: {test_response_1}")
predictions_1 = predict_misconceptions_final(test_response_1)

if predictions_1:
    print("\nPredicted Misconceptions (Response 1):")
    for pred in predictions_1:
        print(f"  - Label: {pred['label']}")
        print(f"    Score: {pred['score']:.4f} (Calibrated Thresh: {pred['threshold']:.4f})")
else:
    print("No misconceptions detected for Response 1.")

print(f"\nStudent Response 2: {test_response_2}")
predictions_2 = predict_misconceptions_final(test_response_2)

if predictions_2:
    print("\nPredicted Misconceptions (Response 2):")
    for pred in predictions_2:
        print(f"  - Label: {pred['label']}")
        print(f"    Score: {pred['score']:.4f} (Calibrated Thresh: {pred['threshold']:.4f})")
else:
    print("No misconceptions detected for Response 2.")
