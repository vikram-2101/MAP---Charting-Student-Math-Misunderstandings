pip install --upgrade --force-reinstall pyarrow datasets

!pip install -q sentence-transformers transformers datasets rank_bm25 faiss-cpu

import os
os.environ["CUDA_VISIBLE_DEVICES"] = "0,1"

VER=1
#model_name = "google/gemma-2-9b-it"
# model_name = "google/gemma-2b-it"
EPOCHS = 2

DIR = f"ver_{VER}"
os.makedirs(DIR, exist_ok=True)

import pandas as pd, numpy as np
from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
train = pd.read_csv('/content/train.csv')
train.Misconception = train.Misconception.fillna('NA')
train['target'] = train.Category+":"+train.Misconception
train['label'] = le.fit_transform(train['target'])
target_classes = le.classes_
n_classes = len(target_classes)
print(f"Train shape: {train.shape} with {n_classes} target classes")
train.sample(5)

#Powerful Feature Engineer

idx = train.apply(lambda row: row.Category.split('_')[0],axis=1)=='True'
correct = train.loc[idx].copy()
correct['c'] = correct.groupby(['QuestionId','MC_Answer']).MC_Answer.transform('count')
correct = correct.sort_values('c',ascending=False)
correct = correct.drop_duplicates(['QuestionId'])
correct = correct[['QuestionId','MC_Answer']]
correct['is_correct'] = 1

train = train.merge(correct, on=['QuestionId','MC_Answer'], how='left')
train.is_correct = train.is_correct.fillna(0)

#Question EDA
from IPython.display import display, Math, Latex

# GET ANSWER CHOICES
tmp = train.groupby(['QuestionId','MC_Answer']).size().reset_index(name='count')
tmp['rank'] = tmp.groupby('QuestionId')['count'].rank(method='dense', ascending=False).astype(int) - 1
tmp = tmp.drop('count',axis=1)
tmp = tmp.sort_values(['QuestionId','rank'])

# DISPLAY QUESTION AND ANSWER CHOICES
Q = tmp.QuestionId.unique()
for q in Q:
    question = train.loc[train.QuestionId==q].iloc[0].QuestionText
    choices = tmp.loc[tmp.QuestionId==q].MC_Answer.values
    labels="ABCD"
    choice_str = " ".join([f"({labels[i]}) {choice}" for i, choice in enumerate(choices)])

    print()
    display(Latex(f"QuestionId {q}: {question}") )
    display(Latex(f"MC Answers: {choice_str}"))

import torch
from transformers import AutoTokenizer
from sklearn.model_selection import train_test_split
from datasets import Dataset
import numpy as np


model_name = "microsoft/deberta-v3-base"  # or "roberta-base", "deberta-v3-base"
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
from transformers import AutoModelForSequenceClassification
model = AutoModelForSequenceClassification.from_pretrained(
    model_name,
    num_labels=n_classes
).to(device)

tokenizer = AutoTokenizer.from_pretrained(model_name)
MAX_LEN = 256

def format_input(row):
    x = "Yes"
    if not row['is_correct']:
        x = "No"
    return (
        f"Question: {row['QuestionText']}\n"
        f"Answer: {row['MC_Answer']}\n"
        f"Correct? {x}\n"
        f"Student Explanation: {row['StudentExplanation']}"
    )

train['text'] = train.apply(format_input,axis=1)
print("Example prompt for our LLM:")
print()
print( train.text.values[0] )

lengths = [len(tokenizer.encode(t, truncation=False)) for t in train["text"]]
import matplotlib.pyplot as plt

plt.hist(lengths, bins=50)
plt.title("Token Length Distribution")
plt.xlabel("Number of tokens")
plt.ylabel("Frequency")
plt.grid(True)
plt.show()

L = (np.array(lengths)>MAX_LEN).sum()
print(f"There are {L} train sample(s) with more than {MAX_LEN} tokens")
np.sort( lengths )

import torch
from transformers import AutoTokenizer
from sklearn.model_selection import train_test_split
from datasets import Dataset
import numpy as np

train_df, val_df = train_test_split(train, test_size=0.2, random_state=42)

# Convert to Hugging Face Dataset
COLS = ['text','label']
train_ds = Dataset.from_pandas(train_df[COLS])
val_ds = Dataset.from_pandas(val_df[COLS])

# Tokenization function
def tokenize(batch):
    return tokenizer(batch["text"], padding="max_length", truncation=True, max_length=256)

train_ds = train_ds.map(tokenize, batched=True)
val_ds = val_ds.map(tokenize, batched=True)

# Set format for PyTorch
columns = ['input_ids', 'attention_mask', 'label']
train_ds.set_format(type='torch', columns=columns)
val_ds.set_format(type='torch', columns=columns)

from transformers import TrainingArguments, Trainer

training_args = TrainingArguments(
    output_dir=f"./{DIR}",
    do_train=True,
    do_eval=True,
    eval_strategy="steps",   # <- use this if your transformers is recent
    save_strategy="steps",
    num_train_epochs=EPOCHS,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=16,
    learning_rate=2e-5,
    logging_dir="./logs",
    logging_steps=50,
    save_steps=200,
    eval_steps=200,
    save_total_limit=1,
    metric_for_best_model="map@3",
    greater_is_better=True,
    load_best_model_at_end=True,
    report_to="none",
    bf16=False,
    fp16=True,
)

from sklearn.metrics import average_precision_score

def compute_map3(eval_pred):
    logits, labels = eval_pred
    probs = torch.nn.functional.softmax(torch.tensor(logits), dim=-1).numpy()

    top3 = np.argsort(-probs, axis=1)[:, :3]  # Top 3 predictions
    match = (top3 == labels[:, None])

    # Compute MAP@3 manually
    map3 = 0
    for i in range(len(labels)):
        if match[i, 0]:
            map3 += 1.0
        elif match[i, 1]:
            map3 += 1.0 / 2
        elif match[i, 2]:
            map3 += 1.0 / 3
    return {"map@3": map3 / len(labels)}

# Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_ds,
    eval_dataset=val_ds,
    tokenizer=tokenizer,
    compute_metrics=compute_map3,
)

trainer.save_model(f"ver_{VER}")
tokenizer.save_pretrained(f"ver_{VER}")

test = pd.read_csv('/content/test.csv')
print( test.shape )
test.sample(2)

test = test.merge(correct, on=['QuestionId','MC_Answer'], how='left')
test.is_correct = test.is_correct.fillna(0)

test['text'] = test.apply(format_input,axis=1)

test.head()

ds_test = Dataset.from_pandas(test[['text']])
ds_test = ds_test.map(tokenize, batched=True)

predictions = trainer.predict(ds_test)
probs = torch.nn.functional.softmax(torch.tensor(predictions.predictions), dim=1).numpy()

# Get top 3 predicted class indices
top3 = np.argsort(-probs, axis=1)[:, :3]   # shape: [num_samples, 3]

# Decode numeric class indices to original string labels
flat_top3 = top3.flatten()
decoded_labels = le.inverse_transform(flat_top3)
top3_labels = decoded_labels.reshape(top3.shape)

# Join 3 labels per row with space
joined_preds = [" ".join(row) for row in top3_labels]

# Save submission
sub = pd.DataFrame({
    "row_id": test.row_id.values,
    "Category:Misconception": joined_preds
})
sub.to_csv("submission10.csv", index=False)
sub.head()

# =====================
# Stage 1: Retrieval
# =====================
from rank_bm25 import BM25Okapi
from sentence_transformers import SentenceTransformer, util

# Build misconception knowledge store
misconceptions = list(le.classes_)  # ["Category:Misconception", ...]
miscon_store = pd.DataFrame({"misconception": misconceptions})

# Sentence embeddings model for retrieval
retriever = SentenceTransformer("sentence-transformers/all-mpnet-base-v2")

# Encode misconceptions
miscon_embs = retriever.encode(miscon_store.misconception.tolist(), convert_to_tensor=True)

# BM25 tokenization
tokenized_miscons = [m.split() for m in miscon_store.misconception]
bm25 = BM25Okapi(tokenized_miscons)

def retrieve_candidates(text, top_k=20):
    """Retrieve top-k misconceptions using Bi-encoder + BM25 hybrid."""
    # Bi-encoder
    query_emb = retriever.encode(text, convert_to_tensor=True)
    bi_scores = util.cos_sim(query_emb, miscon_embs)[0].cpu().numpy()

    # BM25
    bm25_scores = bm25.get_scores(text.split())

    # Hybrid score = weighted sum
    scores = 0.7 * bi_scores + 0.3 * bm25_scores
    topk_idx = scores.argsort()[-top_k:][::-1]
    return miscon_store.iloc[topk_idx].misconception.values

# =====================
# Stage 2: Cross-Encoder
# =====================
from transformers import AutoModelForSequenceClassification

cross_encoder = AutoModelForSequenceClassification.from_pretrained(
   model_name,
    num_labels=n_classes
).to(device)  # your flan-t5-large fine-tuned classifier

def rerank_with_cross_encoder(student_text, candidates):
    """Re-rank candidates using cross-encoder."""
    pairs = [
        f"Question: {student_text}\nCandidate Misconception: {cand}"
        for cand in candidates
    ]

    inputs = tokenizer(
    pairs,
    padding=True,
    truncation=True,
    return_tensors="pt"
).to(device)
    with torch.no_grad():
        logits = cross_encoder(**inputs).logits
        probs = torch.nn.functional.softmax(logits, dim=-1)[:, 1]  # probability of match

    ranked = sorted(zip(candidates, probs.cpu().numpy()), key=lambda x: -x[1])
    return ranked[:3]  # return top-3

# =====================
# Apply on Test Set
# =====================
predictions = []
for _, row in test.iterrows():
    student_text = row["text"]

    # Stage 1: retrieval
    candidates = retrieve_candidates(student_text, top_k=20)

    # Stage 2: re-ranking
    top3 = rerank_with_cross_encoder(student_text, candidates)
    top3_labels = [c for c, _ in top3]

    predictions.append(" ".join(top3_labels))

# Save submission
sub = pd.DataFrame({
    "row_id": test.row_id.values,
    "Category:Misconception": predictions
})
sub.to_csv("submission_pipeline.csv", index=False)
print(sub.head())

import numpy as np
import torch

def mapk(y_true, y_pred, k=3):
    """
    Compute Mean Average Precision at k (MAP@k).
    y_true: [n_samples] true class indices
    y_pred: [n_samples, n_classes] predicted probabilities
    """
    topk = np.argsort(-y_pred, axis=1)[:, :k]
    score = 0.0
    for i, true in enumerate(y_true):
        if true in topk[i]:
            rank = np.where(topk[i] == true)[0][0] + 1
            score += 1.0 / rank
    return score / len(y_true)

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    probs = torch.nn.functional.softmax(torch.tensor(logits), dim=-1).numpy()
    preds = np.argmax(probs, axis=1)

    # Accuracy (Top-1)
    acc = (preds == labels).mean()

    # MAP@3
    map3_score = mapk(labels, probs, k=3)

    return {"accuracy": acc, "map@3": map3_score}

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_ds,
    eval_dataset=val_ds,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,  # <-- new
)

trainer.train()
trainer.evaluate()

from google.colab import drive
drive.mount('/content/drive')

output_dir = "/content/drive/MyDrive/my_nlp_model"  # choose any folder name

trainer.save_model(output_dir)

#Prediction
from google.colab import drive
drive.mount('/content/drive')

from transformers import AutoModelForSequenceClassification, AutoTokenizer
import torch
import pandas as pd

# Path to the saved model in Drive
model_path = "/content/drive/MyDrive/my_nlp_model"

model = AutoModelForSequenceClassification.from_pretrained(model_path)
tokenizer = AutoTokenizer.from_pretrained(model_path)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)
model.eval()

te_df = pd.read_csv("/content/test.csv")

# Change this to the actual text column name
texts = te_df["StudentExplanation"].tolist()

from torch.utils.data import DataLoader, TensorDataset

batch_size = 16
all_preds = []

for i in range(0, len(texts), batch_size):
    batch_texts = texts[i:i+batch_size]

    # Tokenize
    inputs = tokenizer(
        batch_texts,
        padding=True,
        truncation=True,
        max_length=256,
        return_tensors="pt"
    ).to(device)

    # Predict
    with torch.no_grad():
        outputs = model(**inputs)
        logits = outputs.logits
        preds = torch.argmax(logits, dim=-1)

    all_preds.extend(preds.cpu().numpy())

id2label = model.config.id2label  # <- correct mapping used during training
te_df["predicted_label"] = [id2label[int(p)] for p in all_preds]

# Combine Category and Misconception into a single label
train_df['combined_label'] = train_df.apply(
    lambda x: f"{x['Category']}:{x['Misconception'] if pd.notna(x['Misconception']) else 'NA'}", axis=1
)

# Get unique combined labels and create mapping
unique_labels = sorted(train_df['combined_label'].unique())
label2id = {label: i for i, label in enumerate(unique_labels)}
id2label = {i: label for label, i in label2id.items()}

print("Total labels:", len(unique_labels))
print(list(id2label.items())[:66])  # preview first 10
# Combine Category and Misconception into a single label
train_df['combined_label'] = train_df.apply(
    lambda x: f"{x['Category']}:{x['Misconception'] if pd.notna(x['Misconception']) else 'NA'}", axis=1
)

# Get unique combined labels and create mapping
unique_labels = sorted(train_df['combined_label'].unique())
label2id = {label: i for i, label in enumerate(unique_labels)}
id2label = {i: label for label, i in label2id.items()}

print("Total labels:", len(unique_labels))
print(list(id2label.items())[:66])  # preview first 10
# Combine Category and Misconception into a single label
train_df['combined_label'] = train_df.apply(
    lambda x: f"{x['Category']}:{x['Misconception'] if pd.notna(x['Misconception']) else 'NA'}", axis=1
)

# Get unique combined labels and create mapping
unique_labels = sorted(train_df['combined_label'].unique())
label2id = {label: i for i, label in enumerate(unique_labels)}
id2label = {i: label for label, i in label2id.items()}

print("Total labels:", len(unique_labels))
print(list(id2label.items())[:66])  # preview first 10
# Combine Category and Misconception into a single label
train_df['combined_label'] = train_df.apply(
    lambda x: f"{x['Category']}:{x['Misconception'] if pd.notna(x['Misconception']) else 'NA'}", axis=1
)

# Get unique combined labels and create mapping
unique_labels = sorted(train_df['combined_label'].unique())
label2id = {label: i for i, label in enumerate(unique_labels)}
id2label = {i: label for label, i in label2id.items()}

print("Total labels:", len(unique_labels))
print(list(id2label.items())[:66])  # preview first 10
# Combine Category and Misconception into a single label
train_df['combined_label'] = train_df.apply(
    lambda x: f"{x['Category']}:{x['Misconception'] if pd.notna(x['Misconception']) else 'NA'}", axis=1
)

# Get unique combined labels and create mapping
unique_labels = sorted(train_df['combined_label'].unique())
label2id = {label: i for i, label in enumerate(unique_labels)}
id2label = {i: label for label, i in label2id.items()}

print("Total labels:", len(unique_labels))
print(list(id2label.items())[:66])  # preview first 10
# Combine Category and Misconception into a single label
train_df['combined_label'] = train_df.apply(
    lambda x: f"{x['Category']}:{x['Misconception'] if pd.notna(x['Misconception']) else 'NA'}", axis=1
)

# Get unique combined labels and create mapping
unique_labels = sorted(train_df['combined_label'].unique())
label2id = {label: i for i, label in enumerate(unique_labels)}
id2label = {i: label for label, i in label2id.items()}

print("Total labels:", len(unique_labels))
print(list(id2label.items())[:66])  # preview first 10
# Combine Category and Misconception into a single label
train_df['combined_label'] = train_df.apply(
    lambda x: f"{x['Category']}:{x['Misconception'] if pd.notna(x['Misconception']) else 'NA'}", axis=1
)

# Get unique combined labels and create mapping
unique_labels = sorted(train_df['combined_label'].unique())
label2id = {label: i for i, label in enumerate(unique_labels)}
id2label = {i: label for label, i in label2id.items()}

print("Total labels:", len(unique_labels))
print(list(id2label.items())[:66])  # preview first 10
# Combine Category and Misconception into a single label
train_df['combined_label'] = train_df.apply(
    lambda x: f"{x['Category']}:{x['Misconception'] if pd.notna(x['Misconception']) else 'NA'}", axis=1
)

# Get unique combined labels and create mapping
unique_labels = sorted(train_df['combined_label'].unique())
label2id = {label: i for i, label in enumerate(unique_labels)}
id2label = {i: label for label, i in label2id.items()}

print("Total labels:", len(unique_labels))
print(list(id2label.items())[:66])  # preview first 10
# Combine Category and Misconception into a single label
train_df['combined_label'] = train_df.apply(
    lambda x: f"{x['Category']}:{x['Misconception'] if pd.notna(x['Misconception']) else 'NA'}", axis=1
)

# Get unique combined labels and create mapping
unique_labels = sorted(train_df['combined_label'].unique())
label2id = {label: i for i, label in enumerate(unique_labels)}
id2label = {i: label for label, i in label2id.items()}

print("Total labels:", len(unique_labels))
print(list(id2label.items())[:66])  # preview first 10
# Combine Category and Misconception into a single label
train_df['combined_label'] = train_df.apply(
    lambda x: f"{x['Category']}:{x['Misconception'] if pd.notna(x['Misconception']) else 'NA'}", axis=1
)

# Get unique combined labels and create mapping
unique_labels = sorted(train_df['combined_label'].unique())
label2id = {label: i for i, label in enumerate(unique_labels)}
id2label = {i: label for label, i in label2id.items()}

print("Total labels:", len(unique_labels))
print(list(id2label.items())[:66])  # preview first 10
# Combine Category and Misconception into a single label
train_df['combined_label'] = train_df.apply(
    lambda x: f"{x['Category']}:{x['Misconception'] if pd.notna(x['Misconception']) else 'NA'}", axis=1
)

# Get unique combined labels and create mapping
unique_labels = sorted(train_df['combined_label'].unique())
label2id = {label: i for i, label in enumerate(unique_labels)}
id2label = {i: label for label, i in label2id.items()}

print("Total labels:", len(unique_labels))
print(list(id2label.items())[:66])  # preview first 10
# Combine Category and Misconception into a single label
train_df['combined_label'] = train_df.apply(
    lambda x: f"{x['Category']}:{x['Misconception'] if pd.notna(x['Misconception']) else 'NA'}", axis=1
)

# Get unique combined labels and create mapping
unique_labels = sorted(train_df['combined_label'].unique())
label2id = {label: i for i, label in enumerate(unique_labels)}
id2label = {i: label for label, i in label2id.items()}

print("Total labels:", len(unique_labels))
print(list(id2label.items())[:66])  # preview first 10

print(te_df[["StudentExplanation", "predicted_label"]].head())

te_df["predicted_label"] = all_preds
print(te_df.head())

import pandas as pd

train_df = pd.read_csv("/content/train.csv")
unique_labels = sorted(train_df["Category"].unique())
id2label = {i: label for i, label in enumerate(unique_labels)}

