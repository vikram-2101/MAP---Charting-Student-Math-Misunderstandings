#this is the new and correct notebook
import pandas as pd
import numpy as np
import re
import os
import torch
from sklearn.preprocessing import MultiLabelBinarizer
from sklearn.model_selection import train_test_split
from torch.utils.data import Dataset
from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer
from torch.nn import BCEWithLogitsLoss
from sklearn.metrics import average_precision_score # For standard AP score




###6
# --- 3. Pipeline Execution ---

def run_trainer_pipeline():
    # --- Load and Prepare Data ---
    df_responses = load_data(TRAIN_FILE, is_training=True)
    if df_responses is None: return

    # --- Multi-Label Encoding ---
    mlb = MultiLabelBinarizer()
    Y_labels = mlb.fit_transform(df_responses['labels'])
    num_labels = len(mlb.classes_)

    # --- Split Data (Training is easier without stratification here) ---
    X_train, X_val, Y_train, Y_val = train_test_split(
        df_responses['input_text'].tolist(), Y_labels, test_size=0.1, random_state=42
    )

    # --- Tokenization ---
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)

    train_encodings = tokenizer(
        X_train, truncation=True, padding='max_length', max_length=MAX_LEN, return_tensors='pt'
    )
    val_encodings = tokenizer(
        X_val, truncation=True, padding='max_length', max_length=MAX_LEN, return_tensors='pt'
    )

#8

def generate_submission(trainer, mlb, test_path, tokenizer, output_filename='submission_deberta_trainer.csv'):
    """Generates the final submission file using the Hugging Face Trainer."""

    df_test = load_data(test_path, is_training=False)

    X_test = df_test['input_text'].tolist()
    test_encodings = tokenizer(
        X_test, truncation=True, padding='max_length', max_length=MAX_LEN, return_tensors='pt'
    )
    test_dataset = MisconceptionDataset(test_encodings)

    # Predict logits using the best model loaded by the Trainer
    raw_predictions = trainer.predict(test_dataset).predictions

    # Convert logits to probabilities
    probabilities = torch.sigmoid(torch.tensor(raw_predictions)).numpy()

    # Get indices of top 3 probability predictions
    # argsort[::-1] gives descending indices, [:3] takes the top 3
    top_3_indices = np.argsort(probabilities, axis=1)[:, ::-1][:, :3]

    predictions = []

    # Map the indices back to the actual class names (Category:Misconception)
    for row in top_3_indices:
        labels = [mlb.classes_[i] for i in row]
        predictions.append(' '.join(labels))

    # Create the submission DataFrame
    submission_df = pd.DataFrame({
        'row_id': df_test['row_id'],
        'Category:Misconception': predictions
    })

    submission_df.to_csv(output_filename, index=False)
    print(f"Submission file saved successfully to {output_filename}")


if __name__ == "__main__":
    run_trainer_pipeline()


# --- 2. Custom Dataset and Model Utils ---

class MisconceptionDataset(Dataset):
    """PyTorch Dataset compatible with Hugging Face Trainer."""
    def _init_(self, encodings, labels=None):
        self.encodings = encodings
        self.labels = labels

    def _getitem_(self, idx):
        item = {key: val[idx].clone().detach() for key, val in self.encodings.items()}
        
        if self.labels is not None:
            # Use float for BCEWithLogitsLoss
            item['labels'] = torch.tensor(self.labels[idx], dtype=torch.float)
            
        return item

    def _len_(self):
        return len(self.encodings['input_ids'])

3.
# --- 1. Data Preparation and Feature Engineering ---

def clean_text(text):
    """Basic text cleaning: lowercase and remove non-alphanumeric characters."""
    if isinstance(text, str):
        # Decode common LaTeX/unicode, clean, and strip excess whitespace
        text = text.lower()
        text = re.sub(r'\s+', ' ', text).strip()
        return text
    return ""

def format_input(row):
    """
    Combines all relevant context into a single input string for the model.
    This mimics the successful strategy of providing full context.
    """
    # Using a clear separator for the model
    return (
        f"Question: {clean_text(row['QuestionText'])} \n"
        f"Answer: {clean_text(row['MC_Answer'])} \n"
        f"Explanation: {clean_text(row['StudentExplanation'])}"
    )

def load_data(path, is_training=True):
    """Loads and prepares data, including feature engineering."""
    try:
        df = pd.read_csv(path)
    except FileNotFoundError:
        print(f"Error: File not found at {path}")
        return None

    # Rename columns for consistency
    df.rename(columns={'StudentExplanation': 'StudentExplanation', 
                      'Misconception': 'misconception_name'}, inplace=True)
    
    # Generate the rich input feature
    df['input_text'] = df.apply(format_input, axis=1)
    
    if is_training:
        # Create the combined target label
        df['target_label'] = df['Category'].astype(str) + ':' + df['misconception_name'].fillna('NA').astype(str)
        # Group to get a list of all targets per student response (Multi-Label)
        df_responses = df.groupby('row_id')['target_label'].apply(list).reset_index(name='labels')
        
        # Merge back the unique input text for each row_id
        df_responses = df_responses.merge(df[['row_id', 'input_text']].drop_duplicates(subset=['row_id']), on='row_id')
        
        return df_responses
    
    return df
