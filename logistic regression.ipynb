{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":104383,"databundleVersionId":12957508,"sourceType":"competition"},{"sourceId":13078696,"sourceType":"datasetVersion","datasetId":8283324},{"sourceId":13354645,"sourceType":"datasetVersion","datasetId":8470274},{"sourceId":13370013,"sourceType":"datasetVersion","datasetId":8481899}],"dockerImageVersionId":31089,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport os, shutil\n\nBASE_MODEL = \"microsoft/deberta-v3-base\"\n\n# Save the model and tokenizer locally\ntokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\nmodel = AutoModelForSequenceClassification.from_pretrained(BASE_MODEL, num_labels=8)\n\nos.makedirs(\"/kaggle/working/deberta_model\", exist_ok=True)\ntokenizer.save_pretrained(\"/kaggle/working/deberta_model\")\nmodel.save_pretrained(\"/kaggle/working/deberta_model\")\n\nprint(\"‚úÖ Model and tokenizer saved at /kaggle/working/deberta_model\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T10:47:58.537764Z","iopub.execute_input":"2025-10-14T10:47:58.538498Z","iopub.status.idle":"2025-10-14T10:48:45.913615Z","shell.execute_reply.started":"2025-10-14T10:47:58.538464Z","shell.execute_reply":"2025-10-14T10:48:45.912117Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9bf4eddaeed49b5b0082293983313ef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/579 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c4b1762871c249a0b7e5d9b61ef21c1c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e2977edac6a04eeeb90efcdcbf71f9ea"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n2025-10-14 10:48:23.918805: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1760438904.356175      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1760438904.457592      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/371M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"91ea8c2fb73840978bc076d68685e797"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/371M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b4d60a4b012445eea69eeecbdee9a2db"}},"metadata":{}},{"name":"stderr","text":"Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"‚úÖ Model and tokenizer saved at /kaggle/working/deberta_model\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport re\nimport os\nimport torch\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import Dataset\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding\nfrom sklearn.metrics import average_precision_score\nfrom tqdm.auto import tqdm\nos.environ[\"WANDB_DISABLED\"] = \"true\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T10:57:46.293884Z","iopub.execute_input":"2025-10-14T10:57:46.294170Z","iopub.status.idle":"2025-10-14T10:57:46.298921Z","shell.execute_reply.started":"2025-10-14T10:57:46.294146Z","shell.execute_reply":"2025-10-14T10:57:46.298351Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# --- Configuration ---\nKAGGLE_DATA_PATH = \"/kaggle/input/map-charting-student-math-misunderstandings/\"\nTRAIN_FILE = os.path.join(KAGGLE_DATA_PATH, \"train.csv\")\nTEST_FILE = os.path.join(KAGGLE_DATA_PATH, \"test.csv\")\nSUBMISSION_FILE = \"submission.csv\"\n\nMODEL_NAME = 'microsoft/deberta-v3-base'\nMAX_LEN = 512\nNUM_EPOCHS = 3\nLR = 2e-5\nPER_DEVICE_BATCH_SIZE = 8\nSEED = 42\nN_FOLDS = 5\n\ntorch.manual_seed(SEED)\nnp.random.seed(SEED)\n\n# --- Data Preparation ---\ndef clean_text(text):\n    if isinstance(text, str):\n        text = text.lower()\n        text = re.sub(r'\\s+', ' ', text).strip()\n        return text\n    return \"\"\n\ndef format_input(row):\n    return (\n        f\"Question: {clean_text(row['QuestionText'])} \\n\"\n        f\"Answer: {clean_text(row['MC_Answer'])} \\n\"\n        f\"Explanation: {clean_text(row['StudentExplanation'])}\"\n    )\n\n\ndef load_data(path, is_training=True):\n    df = pd.read_csv(path)\n    df['input_text'] = df.apply(format_input, axis=1)\n    if is_training:\n        df['target_label'] = df['Category'].astype(str) + ':' + df['Misconception'].fillna('NA').astype(str)\n        df_responses = df.groupby('row_id')['target_label'].apply(list).reset_index(name='labels')\n        df_responses = df_responses.merge(df[['row_id', 'input_text']].drop_duplicates(subset=['row_id']), on='row_id')\n        return df_responses\n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T11:04:17.824566Z","iopub.execute_input":"2025-10-14T11:04:17.824846Z","iopub.status.idle":"2025-10-14T11:04:17.833280Z","shell.execute_reply.started":"2025-10-14T11:04:17.824826Z","shell.execute_reply":"2025-10-14T11:04:17.832464Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"# --- Dataset & Metrics ---\nclass MisconceptionDataset(Dataset):\n    def __init__(self, encodings, labels=None):\n        self.encodings = encodings\n        self.labels = labels\n    def __getitem__(self, idx):\n        item = {key: val[idx].clone().detach() for key, val in self.encodings.items()}\n        if self.labels is not None:\n            item['labels'] = torch.tensor(self.labels[idx], dtype=torch.float)\n        return item\n    def __len__(self):\n        return len(self.encodings['input_ids'])\n\ndef map_at_k(y_true, y_pred_proba, k=3):\n    avg_precisions = []\n    sorted_pred_indices = np.argsort(y_pred_proba, axis=1)[:, ::-1]\n    for i in range(y_true.shape[0]):\n        true_labels = np.where(y_true[i] == 1)[0]\n        if len(true_labels) == 0: continue\n        top_k_pred_indices = sorted_pred_indices[i, :k]\n        running_correct = 0\n        total_precision = 0\n        remaining_true = set(true_labels)\n        for rank, pred_idx in enumerate(top_k_pred_indices, 1):\n            if pred_idx in remaining_true:\n                running_correct += 1\n                total_precision += (running_correct / rank)\n                remaining_true.remove(pred_idx)\n                if not remaining_true: break\n        if running_correct > 0:\n            avg_precisions.append(total_precision / len(true_labels))\n    return np.mean(avg_precisions) if avg_precisions else 0.0\n\ndef compute_metrics(p, mlb_classes):\n    logits = p.predictions\n    probabilities = torch.sigmoid(torch.tensor(logits)).numpy()\n    y_true = p.label_ids\n    map3_score = map_at_k(y_true, probabilities, k=3)\n    macro_ap = average_precision_score(y_true, probabilities, average='macro')\n    return {'map3_score': map3_score, 'macro_ap': macro_ap}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T11:04:20.738908Z","iopub.execute_input":"2025-10-14T11:04:20.739411Z","iopub.status.idle":"2025-10-14T11:04:20.756389Z","shell.execute_reply.started":"2025-10-14T11:04:20.739377Z","shell.execute_reply":"2025-10-14T11:04:20.755506Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"\n\n# --- K-Fold Training with Resume + MAP Evaluation ---\ndef run_kfold_pipeline():\n    print(\"--- Loading and preparing data ---\")\n    df_responses = load_data(TRAIN_FILE, is_training=True)\n    if df_responses is None:\n        return\n\n    mlb = MultiLabelBinarizer()\n    Y_labels = mlb.fit_transform(df_responses['labels'])\n    num_labels = len(mlb.classes_)\n    print(f\"Total labels found: {num_labels}\")\n\n    kf = KFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n    all_test_preds = []\n    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n\n    for fold, (train_index, val_index) in enumerate(kf.split(df_responses)):\n        fold_dir = f'./deberta_results_fold_{fold}'\n\n        # ‚úÖ Skip already completed folds\n        if os.path.exists(os.path.join(fold_dir, \"completed.txt\")):\n            print(f\"Skipping Fold {fold} ‚Äî already trained.\")\n            continue\n\n        print(f\"\\n--- Starting Fold {fold+1}/{N_FOLDS} ---\")\n        fold_train_df = df_responses.iloc[train_index].reset_index(drop=True)\n        fold_val_df = df_responses.iloc[val_index].reset_index(drop=True)\n        \n        X_train, Y_train = fold_train_df['input_text'].tolist(), Y_labels[train_index]\n        X_val, Y_val = fold_val_df['input_text'].tolist(), Y_labels[val_index]\n\n        train_encodings = tokenizer(X_train, truncation=True, padding='max_length', max_length=MAX_LEN, return_tensors='pt')\n        val_encodings = tokenizer(X_val, truncation=True, padding='max_length', max_length=MAX_LEN, return_tensors='pt')\n\n        train_dataset = MisconceptionDataset(train_encodings, Y_train)\n        val_dataset = MisconceptionDataset(val_encodings, Y_val)\n        data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\n        model = AutoModelForSequenceClassification.from_pretrained(\n            MODEL_NAME, num_labels=num_labels, problem_type=\"multi_label_classification\"\n        )\n\n        training_args = TrainingArguments(\n            output_dir=fold_dir,\n            num_train_epochs=NUM_EPOCHS,\n            per_device_train_batch_size=PER_DEVICE_BATCH_SIZE,\n            per_device_eval_batch_size=PER_DEVICE_BATCH_SIZE * 2,\n            warmup_ratio=0.1,\n            weight_decay=0.01,\n            learning_rate=LR,\n            logging_steps=50,\n            eval_strategy=\"epoch\",\n            save_strategy=\"epoch\",\n            save_total_limit=2,\n            load_best_model_at_end=True,\n            metric_for_best_model='map3_score',\n            greater_is_better=True,\n            fp16=torch.cuda.is_available(),\n            report_to=\"none\"\n        )\n\n        def wrapped_compute_metrics(p):\n            return compute_metrics(p, mlb.classes_)\n\n        trainer = Trainer(\n            model=model,\n            args=training_args,\n            train_dataset=train_dataset,\n            eval_dataset=val_dataset,\n            tokenizer=tokenizer,\n            compute_metrics=wrapped_compute_metrics,\n            data_collator=data_collator,\n        )\n\n        trainer.train()\n\n\nif __name__ == \"__main__\":\n    run_kfold_pipeline()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T11:13:43.462207Z","iopub.execute_input":"2025-10-14T11:13:43.462511Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"--- Loading and preparing data ---\nTotal labels found: 65\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n--- Starting Fold 1/5 ---\n","output_type":"stream"},{"name":"stderr","text":"Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/tmp/ipykernel_36/1802758815.py:65: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='5505' max='5505' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [5505/5505 2:12:00, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Map3 Score</th>\n      <th>Macro Ap</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.023200</td>\n      <td>0.022769</td>\n      <td>0.877577</td>\n      <td>0.281498</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.014600</td>\n      <td>0.015685</td>\n      <td>0.911011</td>\n      <td>0.401557</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.012900</td>\n      <td>0.013676</td>\n      <td>0.924040</td>\n      <td>0.434606</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n--- Starting Fold 2/5 ---\n","output_type":"stream"},{"name":"stderr","text":"Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/tmp/ipykernel_36/1802758815.py:65: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='5505' max='5505' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [5505/5505 2:12:00, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Map3 Score</th>\n      <th>Macro Ap</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.020500</td>\n      <td>0.020032</td>\n      <td>0.884773</td>\n      <td>0.281715</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.017100</td>\n      <td>0.014529</td>\n      <td>0.919136</td>\n      <td>0.414665</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.011600</td>\n      <td>0.013351</td>\n      <td>0.924073</td>\n      <td>0.441155</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n--- Starting Fold 3/5 ---\n","output_type":"stream"},{"name":"stderr","text":"Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/tmp/ipykernel_36/1802758815.py:65: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='5367' max='5505' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [5367/5505 2:05:17 < 03:13, 0.71 it/s, Epoch 2.92/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Map3 Score</th>\n      <th>Macro Ap</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.022200</td>\n      <td>0.020782</td>\n      <td>0.886812</td>\n      <td>0.267555</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.014700</td>\n      <td>0.014797</td>\n      <td>0.921330</td>\n      <td>0.394427</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"# --- Final Prediction Ensemble ---\n    print(\"\\n--- Generating final submission ---\")\n    df_test = load_data(TEST_FILE, is_training=False)\n    X_test = df_test['input_text'].tolist()\n    test_encodings = tokenizer(X_test, truncation=True, padding='max_length', max_length=MAX_LEN, return_tensors='pt')\n    test_dataset = MisconceptionDataset(test_encodings)\n\n    for fold in range(N_FOLDS):\n        fold_dir = f'./deberta_results_fold_{fold}'\n        if not os.path.exists(os.path.join(fold_dir, \"pytorch_model.bin\")):\n            print(f\"Skipping fold {fold} ‚Äî model not found.\")\n            continue\n\n        model = AutoModelForSequenceClassification.from_pretrained(fold_dir)\n        trainer = Trainer(model=model, tokenizer=tokenizer)\n        raw_predictions = trainer.predict(test_dataset).predictions\n        probabilities = torch.sigmoid(torch.tensor(raw_predictions)).numpy()\n        all_test_preds.append(probabilities)\n        del model, trainer\n        torch.cuda.empty_cache()\n\n\n avg_probabilities = np.mean(all_test_preds, axis=0)\n    top_3_indices = np.argsort(avg_probabilities, axis=1)[:, ::-1][:, :3]\n    predictions = [' '.join([mlb.classes_[i] for i in row]) for row in top_3_indices]\n\n    submission_df = pd.DataFrame({\n        'row_id': df_test['row_id'],\n        'Category:Misconception': predictions\n    })\n    submission_df.to_csv(SUBMISSION_FILE, index=False)\n    print(f\"‚úÖ Submission saved to {SUBMISSION_FILE}\")\n    print(submission_df.head())\n\nif __name__ == \"__main__\":\n    run_kfold_pipeline()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ================================================================\n# ‚úÖ Final Cross-Validation + Ensemble + Submission Pipeline\n# ================================================================\nimport os\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom transformers import AutoModelForSequenceClassification, Trainer\nfrom torch.utils.data import Dataset\nfrom sklearn.metrics import average_precision_score\n\n# ================================================================\n# 1Ô∏è‚É£ Configs\n# ================================================================\nN_FOLDS = 5\nMAX_LEN = 512\nBATCH_SIZE = 16\nBASE_MODEL = \"microsoft/deberta-v3-small\"\nTRAIN_FILE = \"/kaggle/input/train.csv\"   # update if different\nTEST_FILE = \"/kaggle/input/test.csv\"     # update if different\nSUBMISSION_FILE = \"submission.csv\"\n\n# ================================================================\n# 2Ô∏è‚É£ Dataset Class\n# ================================================================\nclass MisconceptionDataset(Dataset):\n    def __init__(self, encodings, labels=None):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __len__(self):\n        return len(self.encodings[\"input_ids\"])\n\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        if self.labels is not None:\n            item[\"labels\"] = torch.tensor(self.labels[idx]).float()\n        return item\n\n# ================================================================\n# 3Ô∏è‚É£ MAP@3 Metric Function\n# ================================================================\ndef mapk(actual, predicted, k=3):\n    return np.mean([\n        np.isin(predicted[i][:k], actual[i]).sum() / k\n        for i in range(len(actual))\n    ])\n\n# ================================================================\n# 4Ô∏è‚É£ Main Pipeline\n# ================================================================\ndef run_kfold_pipeline():\n    from sklearn.model_selection import KFold\n    from sklearn.preprocessing import MultiLabelBinarizer\n    from transformers import AutoTokenizer, TrainingArguments\n\n    print(\"\\n--- Loading and preparing data ---\")\n    df = pd.read_csv(TRAIN_FILE)\n    df['labels'] = df['Category:Misconception'].apply(lambda x: x.split())\n    mlb = MultiLabelBinarizer()\n    y = mlb.fit_transform(df['labels'])\n\n    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n    X = df['input_text'].tolist()\n\n    kf = KFold(n_splits=N_FOLDS, shuffle=True, random_state=42)\n    fold = 0\n    all_test_preds = []\n    all_val_scores = []\n\n    for train_index, val_index in kf.split(X):\n        fold_dir = f'./deberta_results_fold_{fold}'\n        if os.path.exists(os.path.join(fold_dir, \"pytorch_model.bin\")):\n            print(f\"Skipping Fold {fold} ‚Äî model already exists.\")\n            fold += 1\n            continue\n\n        print(f\"\\n--- Starting Fold {fold+1}/{N_FOLDS} ---\")\n\n        X_train, X_val = [X[i] for i in train_index], [X[i] for i in val_index]\n        y_train, y_val = y[train_index], y[val_index]\n\n        train_enc = tokenizer(X_train, truncation=True, padding='max_length', max_length=MAX_LEN, return_tensors='pt')\n        val_enc = tokenizer(X_val, truncation=True, padding='max_length', max_length=MAX_LEN, return_tensors='pt')\n\n        train_dataset = MisconceptionDataset(train_enc, y_train)\n        val_dataset = MisconceptionDataset(val_enc, y_val)\n\n        model = AutoModelForSequenceClassification.from_pretrained(BASE_MODEL, num_labels=y.shape[1])\n\n        args = TrainingArguments(\n            output_dir=fold_dir,\n            per_device_train_batch_size=BATCH_SIZE,\n            per_device_eval_batch_size=BATCH_SIZE,\n            evaluation_strategy=\"epoch\",\n            save_strategy=\"epoch\",\n            learning_rate=2e-5,\n            num_train_epochs=3,\n            load_best_model_at_end=True,\n            metric_for_best_model=\"eval_loss\",\n            save_total_limit=1,\n            logging_dir=f\"{fold_dir}/logs\",\n        )\n\n        trainer = Trainer(model=model, args=args, train_dataset=train_dataset, eval_dataset=val_dataset)\n        trainer.train()\n        trainer.save_model(fold_dir)\n        print(f\"‚úÖ Fold {fold} model saved at {fold_dir}\")\n\n        # Local validation MAP@3\n        val_preds = torch.sigmoid(torch.tensor(trainer.predict(val_dataset).predictions)).numpy()\n        top_3 = np.argsort(val_preds, axis=1)[:, ::-1][:, :3]\n        actual_indices = [np.where(row == 1)[0] for row in y_val]\n        score = mapk(actual_indices, top_3, k=3)\n        all_val_scores.append(score)\n        print(f\"üìä Local MAP@3 for fold {fold}: {score:.4f}\")\n\n        del model, trainer\n        torch.cuda.empty_cache()\n        fold += 1\n\n    print(f\"\\nüìà Average Local MAP@3 across folds: {np.mean(all_val_scores):.4f}\")\n\n    # ================================================================\n    # üß† Final Ensemble for Submission\n    # ================================================================\n    print(\"\\n--- Generating final submission ---\")\n    df_test = pd.read_csv(TEST_FILE)\n    X_test = df_test['input_text'].tolist()\n    test_encodings = tokenizer(X_test, truncation=True, padding='max_length', max_length=MAX_LEN, return_tensors='pt')\n    test_dataset = MisconceptionDataset(test_encodings)\n\n    for fold in range(N_FOLDS):\n        fold_dir = f'./deberta_results_fold_{fold}'\n        if not os.path.exists(os.path.join(fold_dir, \"pytorch_model.bin\")):\n            print(f\"Skipping fold {fold} ‚Äî model not found.\")\n            continue\n\n        model = AutoModelForSequenceClassification.from_pretrained(fold_dir)\n        trainer = Trainer(model=model, tokenizer=tokenizer)\n        raw_preds = trainer.predict(test_dataset).predictions\n        probs = torch.sigmoid(torch.tensor(raw_preds)).numpy()\n        all_test_preds.append(probs)\n\n        del model, trainer\n        torch.cuda.empty_cache()\n\n    avg_prob = np.mean(all_test_preds, axis=0)\n    top_3_idx = np.argsort(avg_prob, axis=1)[:, ::-1][:, :3]\n    predictions = [' '.join([mlb.classes_[i] for i in row]) for row in top_3_idx]\n\n    submission_df = pd.DataFrame({\n        'row_id': df_test['row_id'],\n        'Category:Misconception': predictions\n    })\n    submission_df.to_csv(SUBMISSION_FILE, index=False)\n    print(f\"‚úÖ Submission saved to {SUBMISSION_FILE}\")\n    print(submission_df.head())\n\n# ================================================================\n# üöÄ Run\n# ================================================================\nif __name__ == \"__main__\":\n    run_kfold_pipeline()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T11:03:10.512820Z","iopub.execute_input":"2025-10-14T11:03:10.513457Z","iopub.status.idle":"2025-10-14T11:03:10.564742Z","shell.execute_reply.started":"2025-10-14T11:03:10.513432Z","shell.execute_reply":"2025-10-14T11:03:10.563812Z"},"collapsed":true,"jupyter":{"source_hidden":true,"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"\n--- Loading and preparing data ---\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/1486869221.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;31m# ================================================================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m     \u001b[0mrun_kfold_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_36/1486869221.py\u001b[0m in \u001b[0;36mrun_kfold_pipeline\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n--- Loading and preparing data ---\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTRAIN_FILE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Category:Misconception'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0mmlb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultiLabelBinarizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/train.csv'"],"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/kaggle/input/train.csv'","output_type":"error"}],"execution_count":24},{"cell_type":"code","source":"# # ================================================================\n# # üìò Math Misconception Multi-Label Prediction Notebook (Optimized)\n# # ================================================================\n\n# import os\n# import pandas as pd\n# import numpy as np\n# import torch\n# from torch.utils.data import Dataset\n# from transformers import AutoTokenizer, AutoModelForSequenceClassification\n# from transformers import AutoTokenizer\n\n# tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, local_files_only=True)\n\n# # ================================================================\n# # 1Ô∏è‚É£ CONFIGURATION\n# # ================================================================\n\n# MODEL_PATH = \"/kaggle/input/map-deberta-v2-trained-model/deberta_results/checkpoint-6195\"\n# DATA_PATH = \"/kaggle/input/map-charting-student-math-misunderstandings/\"\n# TEST_FILE = os.path.join(DATA_PATH, \"test.csv\")\n# TRAIN_FILE = os.path.join(DATA_PATH, \"train.csv\")\n\n# MAX_LEN = 512\n# TOP_K = 3\n# DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# # ================================================================\n# # 2Ô∏è‚É£ TEXT PREPROCESSING\n# # ================================================================\n\n# def clean_text(text):\n#     if isinstance(text, str):\n#         text = text.lower().strip()\n#         text = ' '.join(text.split())\n#         return text\n#     return \"\"\n\n# def format_input(row):\n#     return f\"Question: {clean_text(row['QuestionText'])}\\n\" \\\n#            f\"Answer: {clean_text(row['MC_Answer'])}\\n\" \\\n#            f\"Explanation: {clean_text(row['StudentExplanation'])}\"\n\n# # ================================================================\n# # 3Ô∏è‚É£ DATASET CLASS\n# # ================================================================\n\n# class MisconceptionDataset(Dataset):\n#     def __init__(self, encodings):\n#         self.encodings = encodings\n\n#     def __len__(self):\n#         return self.encodings['input_ids'].shape[0]\n\n#     def __getitem__(self, idx):\n#         return {key: val[idx] for key, val in self.encodings.items()}\n\n# # ================================================================\n# # 4Ô∏è‚É£ LOAD TRAIN & TEST DATA\n# # ================================================================\n\n# # Load train to rebuild mlb classes (or use saved mlb_classes.npy if available)\n# train_df = pd.read_csv(TRAIN_FILE)\n# train_df['Misconception'] = train_df['Misconception'].fillna('NA')\n# train_df['Target'] = train_df['Category'] + \":\" + train_df['Misconception']\n\n# mlb_labels = [[label] for label in train_df['Target'].tolist()]\n# from sklearn.preprocessing import MultiLabelBinarizer\n# mlb = MultiLabelBinarizer()\n# mlb.fit(mlb_labels)\n# all_labels = mlb.classes_\n\n# # Load and preprocess test data\n# test_df = pd.read_csv(TEST_FILE)\n# test_df['input_text'] = test_df.apply(format_input, axis=1)\n# texts = test_df['input_text'].tolist()\n\n# # ================================================================\n# # CREATE INPUT TEXT FOR TRAINING DATA\n# # ================================================================\n# def clean_text(text):\n#     if isinstance(text, str):\n#         text = text.lower().strip()\n#         text = ' '.join(text.split())\n#         return text\n#     return \"\"\n\n# def format_input(row):\n#     return (\n#         f\"Question: {clean_text(row['QuestionText'])}\\n\"\n#         f\"Answer: {clean_text(row['MC_Answer'])}\\n\"\n#         f\"Explanation: {clean_text(row['StudentExplanation'])}\"\n#     )\n\n# train_df['input_text'] = train_df.apply(format_input, axis=1)\n\n# # ================================================================\n# # 7Ô∏è‚É£.1 Local MAP@3 Evaluation (Optional)\n# # ================================================================\n\n# from sklearn.model_selection import train_test_split\n\n# # Split train data into pseudo-train and pseudo-val (10% val)\n# train_texts = train_df['input_text'].tolist()\n# train_labels = mlb.transform([[label] for label in train_df['Target'].tolist()])\n\n# X_train, X_val, Y_train, Y_val = train_test_split(\n#     train_texts, train_labels, test_size=0.1, random_state=42\n# )\n\n# # Tokenize validation\n# val_encodings = tokenizer(X_val, truncation=True, padding=\"max_length\",\n#                           max_length=MAX_LEN, return_tensors=\"pt\")\n# val_dataset = MisconceptionDataset(val_encodings)\n\n# # Predict on validation set\n# model.eval()\n# val_logits = []\n\n# with torch.no_grad():\n#     for i in range(0, len(val_dataset), batch_size):\n#         batch = {k: v[i:i+batch_size].to(DEVICE) for k, v in val_encodings.items()}\n#         outputs = model(**batch)\n#         val_logits.append(outputs.logits.cpu())\n\n# val_logits = torch.cat(val_logits, dim=0)\n# val_probabilities = torch.sigmoid(val_logits).numpy()\n\n# # Top-3 predictions\n# val_top_indices = np.argsort(val_probabilities, axis=1)[:, ::-1][:, :TOP_K]\n\n# # Compute MAP@3\n# def map_at_k(y_true, y_pred, k=3):\n#     score = 0.0\n#     n = y_true.shape[0]\n#     for i in range(n):\n#         true_labels = np.where(y_true[i]==1)[0]\n#         pred_labels = y_pred[i]\n#         hits = 0\n#         avg_prec = 0.0\n#         for j, p in enumerate(pred_labels):\n#             if p in true_labels:\n#                 hits += 1\n#                 avg_prec += hits / (j + 1)\n#         if len(true_labels) > 0:\n#             avg_prec /= min(len(true_labels), k)\n#         score += avg_prec\n#     return score / n\n\n# val_map3 = map_at_k(Y_val, val_top_indices, k=TOP_K)\n# print(f\"üìä Local MAP@3 score on validation set: {val_map3:.4f}\")\n\n\n# # ================================================================\n# # 5Ô∏è‚É£ TOKENIZATION\n# # ================================================================\n\n# tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, local_files_only=True)\n# encodings = tokenizer(texts,\n#                       truncation=True,\n#                       padding=\"max_length\",\n#                       max_length=MAX_LEN,\n#                       return_tensors=\"pt\")\n# test_dataset = MisconceptionDataset(encodings)\n\n# # ================================================================\n# # 6Ô∏è‚É£ LOAD MODEL\n# # ================================================================\n\n# model = AutoModelForSequenceClassification.from_pretrained(\n#     MODEL_PATH,\n#     local_files_only=True\n# )\n# model.to(DEVICE)\n# model.eval()\n\n# # ================================================================\n# # 7Ô∏è‚É£ PREDICTION\n# # ================================================================\n\n# batch_size = 32\n# all_logits = []\n\n# with torch.no_grad():\n#     for i in range(0, len(test_dataset), batch_size):\n#         batch = {k: v[i:i+batch_size].to(DEVICE) for k, v in encodings.items()}\n#         outputs = model(**batch)\n#         all_logits.append(outputs.logits.cpu())\n\n# logits = torch.cat(all_logits, dim=0)\n# probabilities = torch.sigmoid(logits).numpy()\n\n# # ================================================================\n# # 8Ô∏è‚É£ TOP-K PREDICTIONS & MAP@3 READY\n# # ================================================================\n\n# top_indices = np.argsort(probabilities, axis=1)[:, ::-1][:, :TOP_K]\n# predictions = [\" \".join([all_labels[i] for i in row]) for row in top_indices]\n\n# # ================================================================\n# # 9Ô∏è‚É£ SAVE SUBMISSION\n# # ================================================================\n\n# submission_df = pd.DataFrame({\n#     \"row_id\": test_df[\"row_id\"],\n#     \"Category:Misconception\": predictions\n# })\n\n# submission_file = \"submission.csv\"\n# submission_df.to_csv(submission_file, index=False)\n# print(f\"‚úÖ Submission saved to '{submission_file}'\")\n# print(submission_df.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T03:34:50.516252Z","iopub.execute_input":"2025-10-14T03:34:50.516737Z","iopub.status.idle":"2025-10-14T03:34:50.523787Z","shell.execute_reply.started":"2025-10-14T03:34:50.516711Z","shell.execute_reply":"2025-10-14T03:34:50.523002Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# import pandas as pd\n# import numpy as np\n# import re\n# import torch\n# from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer\n# from sklearn.preprocessing import MultiLabelBinarizer\n\n# # Load the training CSV\n# train_file = '/kaggle/input/map-charting-student-math-misunderstandings/train.csv'\n# df = pd.read_csv(train_file)\n\n# # Recreate the target labels\n# df['target_label'] = df['Category'].astype(str) + ':' + df['Misconception'].fillna('NA').astype(str)\n\n# # Group by row_id to get all labels per response\n# df_responses = df.groupby('row_id')['target_label'].apply(list).reset_index(name='labels')\n\n# # Fit MultiLabelBinarizer\n# mlb = MultiLabelBinarizer()\n# mlb.fit(df_responses['labels'])\n\n# # Now you have mlb.classes_ regenerated\n# mlb_classes = mlb.classes_\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T03:34:50.524978Z","iopub.execute_input":"2025-10-14T03:34:50.525294Z","iopub.status.idle":"2025-10-14T03:34:50.568752Z","shell.execute_reply.started":"2025-10-14T03:34:50.525264Z","shell.execute_reply":"2025-10-14T03:34:50.568155Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# from transformers import Trainer, TrainingArguments\n# import os\n\n# os.environ[\"WANDB_DISABLED\"] = \"true\"  # Disable W&B\n\n\n# # --- Config ---\n# TEST_FILE = '/kaggle/input/map-charting-student-math-misunderstandings/test.csv'\n# MODEL_NAME = '/kaggle/input/map-deberta-v2-trained-model/deberta_results/checkpoint-6195'\n# MAX_LEN = 256\n\n# # --- Text preprocessing ---\n# def clean_text(text):\n#     if isinstance(text, str):\n#         text = text.lower()\n#         text = re.sub(r'\\s+', ' ', text).strip()\n#         return text\n#     return \"\"\n\n# def format_input(row):\n#     return (\n#         f\"Question: {clean_text(row['QuestionText'])} \\n\"\n#         f\"Answer: {clean_text(row['MC_Answer'])} \\n\"\n#         f\"Explanation: {clean_text(row['StudentExplanation'])}\"\n#     )\n\n# def load_data(path, is_training=False):\n#     df = pd.read_csv(path)\n#     df['input_text'] = df.apply(format_input, axis=1)\n#     return df\n\n# # --- Dataset class ---\n# class MisconceptionDataset(torch.utils.data.Dataset):\n#     def __init__(self, encodings):\n#         self.encodings = encodings\n\n#     def __getitem__(self, idx):\n#         return {key: val[idx].clone().detach() for key, val in self.encodings.items()}\n\n#     def __len__(self):\n#         return len(self.encodings['input_ids'])\n\n# # --- Generate submission ---\n# def generate_submission(model_path, test_file, output_file='submission.csv'):\n#     df_test = load_data(test_file)\n#     tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True)\n#     test_encodings = tokenizer(\n#         df_test['input_text'].tolist(),\n#         truncation=True,\n#         padding='max_length',\n#         max_length=MAX_LEN,\n#         return_tensors='pt'\n#     )\n#     test_dataset = MisconceptionDataset(test_encodings)\n\n#     # Load model\n#     model = AutoModelForSequenceClassification.from_pretrained(model_path)\n#     trainer = Trainer(model=model)\n\n#     # Predict\n#     raw_predictions = trainer.predict(test_dataset).predictions\n#     probabilities = torch.sigmoid(torch.tensor(raw_predictions)).numpy()\n\n#     # Top 3 predictions\n#     top_3_indices = np.argsort(probabilities, axis=1)[:, ::-1][:, :3]\n\n#     # Map indices to class names (use the same ML classes as Colab)\n#     # You need to save mlb.classes_ from Colab as a .npy file and load it here\n#     # mlb_classes = np.load('/kaggle/input/mlb-classes/mlb_classes.npy', allow_pickle=True)\n#     predictions = [' '.join([mlb_classes[i] for i in row]) for row in top_3_indices]\n\n#     submission_df = pd.DataFrame({\n#         'row_id': df_test['row_id'],\n#         'Category:Misconception': predictions\n#     })\n#     submission_df.to_csv(output_file, index=False)\n#     print(f\"Submission saved to {output_file}\")\n\n# # --- Run ---\n# if __name__ == \"__main__\":\n#     generate_submission(MODEL_NAME, TEST_FILE)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T03:34:50.673319Z","iopub.execute_input":"2025-10-14T03:34:50.673584Z","iopub.status.idle":"2025-10-14T03:34:50.677948Z","shell.execute_reply.started":"2025-10-14T03:34:50.673562Z","shell.execute_reply":"2025-10-14T03:34:50.677285Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# import os\n# import pandas as pd\n# import numpy as np\n# import torch\n# from transformers import AutoTokenizer, AutoModelForSequenceClassification\n# from torch.utils.data import Dataset\n\n# # ================================================================\n# # 1Ô∏è‚É£ CONFIGURATION\n# # ================================================================\n# MODEL_PATH = \"/kaggle/input/map-deberta-v2-trained-model/deberta_results/checkpoint-6195\"  # Your trained model checkpoint\n# DATA_PATH = \"/kaggle/input/map-charting-student-math-misunderstandings/\"\n# TEST_FILE = os.path.join(DATA_PATH, \"test.csv\")\n# MAX_LEN = 512\n# TOP_K = 3\n\n# # ================================================================\n# # 2Ô∏è‚É£ TEXT PREPROCESSING\n# # ================================================================\n# def clean_text(text):\n#     if isinstance(text, str):\n#         text = text.lower().strip()\n#         text = ' '.join(text.split())\n#         return text\n#     return \"\"\n\n# def format_input(row):\n#     return f\"Question: {clean_text(row['QuestionText'])}\\n\" \\\n#            f\"Answer: {clean_text(row['MC_Answer'])}\\n\" \\\n#            f\"Explanation: {clean_text(row['StudentExplanation'])}\"\n\n# # ================================================================\n# # 3Ô∏è‚É£ DATASET CLASS\n# # ================================================================\n# class MisconceptionDataset(Dataset):\n#     def __init__(self, encodings):\n#         self.encodings = encodings\n\n#     def __len__(self):\n#         return self.encodings['input_ids'].shape[0]\n\n#     def __getitem__(self, idx):\n#         return {key: val[idx] for key, val in self.encodings.items()}\n\n# # ================================================================\n# # 4Ô∏è‚É£ LOAD TEST DATA & TOKENIZE\n# # ================================================================\n# df_test = pd.read_csv(TEST_FILE)\n# df_test['input_text'] = df_test.apply(format_input, axis=1)\n# texts = df_test['input_text'].tolist()\n\n# tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, local_files_only=True)\n# encodings = tokenizer(texts, truncation=True, padding=\"max_length\",\n#                       max_length=MAX_LEN, return_tensors=\"pt\")\n\n# test_dataset = MisconceptionDataset(encodings)\n\n# # ================================================================\n# # 5Ô∏è‚É£ LOAD MODEL\n# # ================================================================\n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# model = AutoModelForSequenceClassification.from_pretrained(\n#     MODEL_PATH,\n#     local_files_only=True\n# )\n# model.to(device)\n# model.eval()\n\n# # ================================================================\n# # 6Ô∏è‚É£ PREDICTION\n# # ================================================================\n# with torch.no_grad():\n#     all_logits = []\n#     batch_size = 32  # adjust based on GPU memory\n#     for i in range(0, len(test_dataset), batch_size):\n#         batch = {k: v[i:i+batch_size].to(device) for k, v in encodings.items()}\n#         outputs = model(**batch)\n#         all_logits.append(outputs.logits.cpu())\n    \n#     logits = torch.cat(all_logits, dim=0)\n#     probabilities = torch.sigmoid(logits).numpy()\n\n# # ================================================================\n# # 7Ô∏è‚É£ MAP INDICES TO LABELS\n# # ================================================================\n# # Rebuild MultiLabelBinarizer to get same class order as training\n# from sklearn.preprocessing import MultiLabelBinarizer\n# train_df = pd.read_csv(os.path.join(DATA_PATH, \"train.csv\"))\n# train_df['Misconception'] = train_df['Misconception'].fillna('NA')\n# train_df['Target'] = train_df['Category'] + \":\" + train_df['Misconception']\n# mlb = MultiLabelBinarizer()\n# mlb.fit([[label] for label in train_df['Target'].tolist()])\n# all_labels = mlb.classes_\n\n# top_indices = np.argsort(probabilities, axis=1)[:, ::-1][:, :TOP_K]\n# predictions = [\" \".join([all_labels[i] for i in row]) for row in top_indices]\n\n# # ================================================================\n# # 8Ô∏è‚É£ SAVE SUBMISSION\n# # ================================================================\n# submission_df = pd.DataFrame({\n#     \"row_id\": df_test[\"row_id\"],\n#     \"Category:Misconception\": predictions\n# })\n\n# submission_file = \"submission.csv\"\n# submission_df.to_csv(submission_file, index=False)\n# print(f\"‚úÖ Submission saved to '{submission_file}'\")\n# print(submission_df.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T03:34:50.679176Z","iopub.execute_input":"2025-10-14T03:34:50.679479Z","iopub.status.idle":"2025-10-14T03:34:54.386465Z","shell.execute_reply.started":"2025-10-14T03:34:50.679462Z","shell.execute_reply":"2025-10-14T03:34:54.385711Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\nSome weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at /kaggle/input/deberta-base-v3 and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"‚úÖ Submission saved to 'submission.csv'\n   row_id                             Category:Misconception\n0   36696  False_Correct:NA False_Misconception:Adding_ac...\n1   36697  False_Misconception:Adding_across False_Correc...\n2   36698  False_Correct:NA False_Misconception:Adding_ac...\n","output_type":"stream"},{"name":"stdout","text":"‚úÖ Submission saved to 'submission.csv'\n   row_id                             Category:Misconception\n0   36696  True_Correct:NA True_Neither:NA True_Misconcep...\n1   36697  False_Misconception:WNB False_Misconception:In...\n2   36698  True_Neither:NA True_Correct:NA True_Misconcep...\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# import pandas as pd\n# import numpy as np\n# import re\n# import os\n# import torch\n# from sklearn.preprocessing import MultiLabelBinarizer\n# from sklearn.model_selection import train_test_split\n# from torch.utils.data import Dataset\n# from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n# from torch.nn import BCEWithLogitsLoss\n# from sklearn.metrics import average_precision_score # For standard AP score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T03:34:54.387233Z","iopub.execute_input":"2025-10-14T03:34:54.387524Z","iopub.status.idle":"2025-10-14T03:34:54.391314Z","shell.execute_reply.started":"2025-10-14T03:34:54.387474Z","shell.execute_reply":"2025-10-14T03:34:54.390552Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# # --- Configuration ---\n# TRAIN_FILE = '/kaggle/input/map-charting-student-math-misunderstandings/train.csv'\n# TEST_FILE = '/kaggle/input/map-charting-student-math-misunderstandings/test.csv'\n# MODEL_NAME = '/kaggle/input/map-deberta-v2-trained-model/deberta_results/checkpoint-6195' # High-performance model\n# MAX_LEN = 256  # Max length for the combined input text\n# NUM_EPOCHS = 3\n# LR = 2e-5","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T03:34:54.391979Z","iopub.execute_input":"2025-10-14T03:34:54.392232Z","iopub.status.idle":"2025-10-14T03:34:54.404893Z","shell.execute_reply.started":"2025-10-14T03:34:54.392206Z","shell.execute_reply":"2025-10-14T03:34:54.404093Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"\n\n# # --- 1. Data Preparation and Feature Engineering ---\n\n# def clean_text(text):\n#     \"\"\"Basic text cleaning: lowercase and remove non-alphanumeric characters.\"\"\"\n#     if isinstance(text, str):\n#         # Decode common LaTeX/unicode, clean, and strip excess whitespace\n#         text = text.lower()\n#         text = re.sub(r'\\s+', ' ', text).strip()\n#         return text\n#     return \"\"\n\n# def format_input(row):\n#     \"\"\"\n#     Combines all relevant context into a single input string for the model.\n#     This mimics the successful strategy of providing full context.\n#     \"\"\"\n#     # Using a clear separator for the model\n#     return (\n#         f\"Question: {clean_text(row['QuestionText'])} \\n\"\n#         f\"Answer: {clean_text(row['MC_Answer'])} \\n\"\n#         f\"Explanation: {clean_text(row['StudentExplanation'])}\"\n#     )\n\n# def load_data(path, is_training=True):\n#     \"\"\"Loads and prepares data, including feature engineering.\"\"\"\n#     try:\n#         df = pd.read_csv(path)\n#     except FileNotFoundError:\n#         print(f\"Error: File not found at {path}\")\n#         return None\n\n#     # Rename columns for consistency\n#     df.rename(columns={'StudentExplanation': 'StudentExplanation', \n#                       'Misconception': 'misconception_name'}, inplace=True)\n    \n#     # Generate the rich input feature\n#     df['input_text'] = df.apply(format_input, axis=1)\n    \n#     if is_training:\n#         # Create the combined target label\n#         df['target_label'] = df['Category'].astype(str) + ':' + df['misconception_name'].fillna('NA').astype(str)\n#         # Group to get a list of all targets per student response (Multi-Label)\n#         df_responses = df.groupby('row_id')['target_label'].apply(list).reset_index(name='labels')\n        \n#         # Merge back the unique input text for each row_id\n#         df_responses = df_responses.merge(df[['row_id', 'input_text']].drop_duplicates(subset=['row_id']), on='row_id')\n        \n#         return df_responses\n    \n#     return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T03:34:54.407378Z","iopub.execute_input":"2025-10-14T03:34:54.408135Z","iopub.status.idle":"2025-10-14T03:34:54.414907Z","shell.execute_reply.started":"2025-10-14T03:34:54.408087Z","shell.execute_reply":"2025-10-14T03:34:54.414267Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"\n\n# # --- 2. Custom Dataset and Model Utils ---\n\n# class MisconceptionDataset(Dataset):\n#     \"\"\"PyTorch Dataset compatible with Hugging Face Trainer.\"\"\"\n#     def __init__(self, encodings, labels=None):\n#         self.encodings = encodings\n#         self.labels = labels\n\n#     def __getitem__(self, idx):\n#         item = {key: val[idx].clone().detach() for key, val in self.encodings.items()}\n        \n#         if self.labels is not None:\n#             # Use float for BCEWithLogitsLoss\n#             item['labels'] = torch.tensor(self.labels[idx], dtype=torch.float)\n            \n#         return item\n\n#     def __len__(self):\n#         return len(self.encodings['input_ids'])\n\n# # MAP@K is essential for local evaluation/monitoring\n# def map_at_k(y_true, y_pred_proba, k=3):\n#     \"\"\"Calculates Mean Average Precision at K (MAP@K) for multi-label classification.\"\"\"\n#     avg_precisions = []\n#     # Sort predictions (high to low) and get the indices\n#     sorted_pred_indices = np.argsort(y_pred_proba, axis=1)[:, ::-1] \n    \n#     for i in range(y_true.shape[0]):\n#         # Get the indices of the true positive labels\n#         true_labels = np.where(y_true[i] == 1)[0]\n#         if len(true_labels) == 0: continue\n        \n#         top_k_pred_indices = sorted_pred_indices[i, :k]\n#         running_correct = 0\n#         total_precision = 0\n#         remaining_true = set(true_labels)\n        \n#         for rank, pred_idx in enumerate(top_k_pred_indices, 1):\n#             if pred_idx in remaining_true:\n#                 running_correct += 1\n#                 total_precision += (running_correct / rank)\n#                 remaining_true.remove(pred_idx)\n#                 if not remaining_true: break\n        \n#         if running_correct > 0:\n#             avg_precisions.append(total_precision / len(true_labels))\n\n#     return np.mean(avg_precisions) if avg_precisions else 0.0\n\n# def compute_metrics(p, mlb_classes):\n#     \"\"\"Custom metric function for Trainer to calculate MAP@3.\"\"\"\n#     logits = p.predictions\n#     # Sigmoid to convert logits to probabilities\n#     probabilities = torch.sigmoid(torch.tensor(logits)).numpy()\n#     y_true = p.label_ids\n    \n#     # Calculate MAP@3 for the competition\n#     map3_score = map_at_k(y_true, probabilities, k=3)\n    \n#     # Calculate standard macro Average Precision for comparison\n#     macro_ap = average_precision_score(y_true, probabilities, average='macro')\n    \n#     return {'map3_score': map3_score, 'macro_ap': macro_ap}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T03:34:54.415688Z","iopub.execute_input":"2025-10-14T03:34:54.416298Z","iopub.status.idle":"2025-10-14T03:34:54.428021Z","shell.execute_reply.started":"2025-10-14T03:34:54.416274Z","shell.execute_reply":"2025-10-14T03:34:54.427360Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# # --- 3. Pipeline Execution ---\n\n# def run_trainer_pipeline():\n#     # --- Load and Prepare Data ---\n#     df_responses = load_data(TRAIN_FILE, is_training=True)\n#     if df_responses is None: return\n\n#     # --- Multi-Label Encoding ---\n#     mlb = MultiLabelBinarizer()\n#     Y_labels = mlb.fit_transform(df_responses['labels'])\n#     num_labels = len(mlb.classes_)\n\n#     # --- Split Data (Training is easier without stratification here) ---\n#     X_train, X_val, Y_train, Y_val = train_test_split(\n#         df_responses['input_text'].tolist(), Y_labels, test_size=0.1, random_state=42\n#     )\n\n#     # --- Tokenization ---\n#     tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n\n#     train_encodings = tokenizer(\n#         X_train, truncation=True, padding='max_length', max_length=MAX_LEN, return_tensors='pt'\n#     )\n#     val_encodings = tokenizer(\n#         X_val, truncation=True, padding='max_length', max_length=MAX_LEN, return_tensors='pt'\n#     )\n\n#     # --- Dataset Creation ---\n#     train_dataset = MisconceptionDataset(train_encodings, Y_train)\n#     val_dataset = MisconceptionDataset(val_encodings, Y_val)\n\n#     # --- Model Loading and Configuration ---\n#     model = AutoModelForSequenceClassification.from_pretrained(\n#         MODEL_NAME,\n#         num_labels=num_labels,\n#         # Set problem type for multi-label classification (uses Sigmoid)\n#         problem_type=\"multi_label_classification\",\n#         # Custom loss function to handle multi-label (BCEWithLogitsLoss)\n#         # loss_function=BCEWithLogitsLoss() # Removed as it's not a valid argument\n#     )\n\n#     # --- Training Arguments ---\n#     training_args = TrainingArguments(\n#         output_dir='./deberta_results',\n#         num_train_epochs=NUM_EPOCHS,\n#         per_device_train_batch_size=16, # Increased batch size for efficiency\n#         per_device_eval_batch_size=16,\n#         warmup_ratio=0.1, # Use 10% of steps for learning rate warmup\n#         weight_decay=0.01,\n#         learning_rate=LR,\n#         logging_steps=50,\n#         eval_strategy=\"epoch\", # Evaluate at the end of each epoch\n#         save_strategy=\"epoch\",\n#         load_best_model_at_end=True, # Load the model with the best validation score\n#         metric_for_best_model='map3_score',\n#         greater_is_better=True,\n#         fp16=torch.cuda.is_available(), # Use mixed precision if GPU is available\n#     )\n\n#     # --- Trainer Initialization and Training ---\n#     print(f\"\\n--- Starting Training DeBERTa-v3 on {torch.device('cuda' if torch.cuda.is_available() else 'cpu')} ---\")\n\n#     # We wrap compute_metrics to pass the classes object to the internal function\n#     def wrapped_compute_metrics(p):\n#         return compute_metrics(p, mlb.classes_)\n\n#     trainer = Trainer(\n#         model=model,\n#         args=training_args,\n#         train_dataset=train_dataset,\n#         eval_dataset=val_dataset,\n#         tokenizer=tokenizer,\n#         compute_metrics=wrapped_compute_metrics,\n#     )\n\n#     trainer.train()\n\n#     # --- Prediction and Submission Generation ---\n#     if os.path.exists(TEST_FILE):\n#         print(\"\\n--- Generating Submission on Test Data ---\")\n#         generate_submission(trainer, mlb, TEST_FILE, tokenizer)\n#     else:\n#         print(f\"Test file {TEST_FILE} not found. Cannot generate submission.\")\n\n\n# def generate_submission(trainer, mlb, test_path, tokenizer, output_filename='submission_deberta_trainer.csv'):\n#     \"\"\"Generates the final submission file using the Hugging Face Trainer.\"\"\"\n\n#     df_test = load_data(test_path, is_training=False)\n\n#     X_test = df_test['input_text'].tolist()\n#     test_encodings = tokenizer(\n#         X_test, truncation=True, padding='max_length', max_length=MAX_LEN, return_tensors='pt'\n#     )\n#     test_dataset = MisconceptionDataset(test_encodings)\n\n#     # Predict logits using the best model loaded by the Trainer\n#     raw_predictions = trainer.predict(test_dataset).predictions\n\n#     # Convert logits to probabilities\n#     probabilities = torch.sigmoid(torch.tensor(raw_predictions)).numpy()\n\n#     # Get indices of top 3 probability predictions\n#     # argsort[::-1] gives descending indices, [:3] takes the top 3\n#     top_3_indices = np.argsort(probabilities, axis=1)[:, ::-1][:, :3]\n\n#     predictions = []\n\n#     # Map the indices back to the actual class names (Category:Misconception)\n#     for row in top_3_indices:\n#         labels = [mlb.classes_[i] for i in row]\n#         predictions.append(' '.join(labels))\n\n#     # Create the submission DataFrame\n#     submission_df = pd.DataFrame({\n#         'row_id': df_test['row_id'],\n#         'Category:Misconception': predictions\n#     })\n\n#     submission_df.to_csv(output_filename, index=False)\n#     print(f\"Submission file saved successfully to {output_filename}\")\n\n\n# if __name__ == \"__main__\":\n#     run_trainer_pipeline()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T03:34:54.428840Z","iopub.execute_input":"2025-10-14T03:34:54.429103Z","iopub.status.idle":"2025-10-14T03:34:54.440709Z","shell.execute_reply.started":"2025-10-14T03:34:54.429079Z","shell.execute_reply":"2025-10-14T03:34:54.440163Z"}},"outputs":[],"execution_count":10}]}