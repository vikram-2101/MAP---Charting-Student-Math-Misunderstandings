# project starts here
import os
import pandas as pd
import numpy as np
import torch
from sklearn.preprocessing import MultiLabelBinarizer
from sklearn.metrics import fbeta_score 
from torch.utils.data import Dataset
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer,
    DataCollatorWithPadding
)

# ================================================================
# 1. CONFIGURATION
# ================================================================

# CHECK THIS PATH: IT MUST MATCH THE SLUG OF THE DATASET YOU ATTACH!
BASE_MODEL_DIR = "/kaggle/input/deberta-starter-cv-0-930/ver_1/best"
KAGGLE_DATA_PATH = "/kaggle/input/map-charting-student-math-misunderstandings/"

MAX_LEN = 512
SEED = 42
TRAIN_BATCH_SIZE = 8
EPOCHS = 3
TOP_K = 3

torch.manual_seed(SEED)
np.random.seed(SEED)

# --- Check for attached datasets and set LOCAL_MODEL_PATH ---
print("--- Checking Kaggle Input Directory Contents ---")
try:
    print(os.listdir("/kaggle/input"))
    
    # ðŸš¨ CRITICAL FIX: Since the model files are likely directly attached
    # at the BASE_MODEL_DIR level, we'll use it directly if it exists.
    if "deberta-v3-small-offline" in os.listdir("/kaggle/input"):
        LOCAL_MODEL_PATH = BASE_MODEL_DIR
    else:
        # Fallback to the subfolder logic (less common for DeBERTa on Kaggle)
        subfolders = [f.path for f in os.scandir(BASE_MODEL_DIR) if f.is_dir()]
        LOCAL_MODEL_PATH = subfolders[0] if subfolders else BASE_MODEL_DIR

except FileNotFoundError:
    print(f"FATAL ERROR: Model directory not found at {BASE_MODEL_DIR}. You MUST click '+ Add Input' and attach the model weights dataset.")
    raise # Stop execution as we cannot run offline without the files.

print(f"ðŸ“ Using local model path: {LOCAL_MODEL_PATH}")
print("----------------------------------------------")

# ================================================================
# 2. DATA LOADING & PREPROCESSING
# ================================================================

train_df = pd.read_csv(os.path.join(KAGGLE_DATA_PATH, "train.csv"))
test_df = pd.read_csv(os.path.join(KAGGLE_DATA_PATH, "test.csv"))

# Create unique target label
train_df['Misconception'] = train_df['Misconception'].fillna('NA')
train_df['Target'] = train_df['Category'] + ":" + train_df['Misconception']
train_targets_lists = [[label] for label in train_df['Target'].tolist()]

mlb = MultiLabelBinarizer()
Y_train = mlb.fit_transform(train_targets_lists).astype(np.float32)
all_labels = mlb.classes_
num_labels = len(all_labels)

def format_input(row):
    return(
        f"Question: {row['QuestionText']}\n"
        f"Answer: {row['MC_Answer']}\n"
        f"Explanation: {row['StudentExplanation']}"
    )

train_df['input_text'] = train_df.apply(format_input, axis=1)
test_df['input_text'] = test_df.apply(format_input, axis=1)
X_train = train_df['input_text'].tolist()
X_test = test_df['input_text'].tolist()

# ================================================================
# 3. TOKENIZATION (OFFLINE)
# ================================================================

# This will now successfully load the tokenizer config and vocabulary offline
tokenizer = AutoTokenizer.from_pretrained(LOCAL_MODEL_PATH, local_files_only=True)

raw_train_encodings = tokenizer(
    X_train, truncation=True, padding="max_length", max_length=MAX_LEN, return_tensors="pt"
)

raw_test_encodings = tokenizer(
    X_test, truncation=True, padding="max_length", max_length=MAX_LEN, return_tensors="pt"
)

# ================================================================
# 4. DATASET CLASS & METRICS (Simplified & Corrected)
# ================================================================

class MisconceptionDataset(Dataset):
    def init(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def getitem(self, idx):
        item = {key: val[idx].clone().detach() for key, val in self.encodings.items()}
        item["labels"] = torch.tensor(self.labels[idx], dtype=torch.float) 
        return item

    def len(self):
        return len(self.encodings["input_ids"])

train_dataset = MisconceptionDataset(raw_train_encodings, Y_train)
dummy_test_labels = np.zeros((len(X_test), num_labels), dtype=np.float32)
test_dataset = MisconceptionDataset(raw_test_encodings, dummy_test_labels)
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

def compute_metrics(p):
    logits = p.predictions
    probas = 1.0 / (1.0 + np.exp(-logits))
    preds = (probas > 0.5).astype(int)
    f2 = fbeta_score(p.label_ids, preds, beta=2, average='micro', zero_division=0)
    return {"f2_score_micro": f2}
